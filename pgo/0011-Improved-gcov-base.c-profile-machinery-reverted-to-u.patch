From e57e0e6f9d6dfa64d2acc5771eeeb5c06c2b9745 Mon Sep 17 00:00:00 2001
From: Jarmo Tiitto <jarmo.tiitto@gmail.com>
Date: Tue, 6 Jun 2017 00:05:14 +0300
Subject: [PATCH 11/21] Improved gcov/base.c profile machinery, reverted to
 using atomics but kept the try-lock to protect multi-RMW seqences.
 gcov_profile_one_value_body() now retries if cmpxchg fails. This reduced
 number of rejected gcda files. Also disabled instrumentation for arch/x86
 since it still produces corrupt profile data.

---
 Makefile           |   2 +-
 arch/x86/Makefile  |   3 ++
 genPGOKernel       |  16 +++++--
 kernel/gcov/base.c | 102 ++++++++++++++++++++++-----------------------
 kernel/gcov/gcov.h |  38 ++++++++++++-----
 5 files changed, 92 insertions(+), 69 deletions(-)

diff --git a/Makefile b/Makefile
index 871071ad87ba..66d91449ad42 100644
--- a/Makefile
+++ b/Makefile
@@ -690,7 +690,7 @@ KBUILD_CFLAGS	+= -fprofile-use=/tmp/kernelPGO -fprofile-dir=/tmp/kernelPGO \
 		-finline-functions -fprofile-reorder-functions -fprofile-values \
 		-freorder-blocks-and-partition -fipa-profile -fvpt \
 		-fipa-cp -fipa-cp-clone -fipa-pta -fpeel-loops \
-		-ftree-partial-pre \
+		-ftree-partial-pre -fprofile-correction \
 		-fschedule-insns -fsched-pressure -fgraphite \
 		-fsched-stalled-insns=4 -fsched-stalled-insns-dep=32
 endif
diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 4c4212d1fbc0..f8027ca1525a 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -253,6 +253,9 @@ ifneq ($(RETPOLINE_CFLAGS),)
 endif
 endif
 
+# Don't instrument anything here.
+GCOV_PROFILE := n
+
 archscripts: scripts_basic
 	$(Q)$(MAKE) $(build)=arch/x86/tools relocs
 
diff --git a/genPGOKernel b/genPGOKernel
index 377771a9104e..05f41bba3012 100755
--- a/genPGOKernel
+++ b/genPGOKernel
@@ -3,7 +3,7 @@
 # and mangle it for consumption by PGO build
 # the kernel is built after this.
 
-KGCCVER=/opt/kgcc-7.0/bin/
+KGCCVER=/opt/kgcc-7.0/bin
 echo 'Initializing kernel PGO optimization..'
 DIR=$PWD
 mkdir -p /tmp/kernelsrc
@@ -43,7 +43,8 @@ if [ ! -f $CHECKPROF/mm/.tmp_mmap.gcda ]; then
     if [ ! -f $CHECKPROF/mm/mmap.gcda  ]; then
         die 'No Kernel profiling data available. Stop.'
     fi
-
+    echo 'Discaring arch/x86/ profile data. (it has a lot of uninstrumentible code)'
+    rm -rf $CHECKPROF/arch/x86
     echo 'Postprocesing profile data with calcsum..'
     find /tmp/kernelPGOdump -name '*.gcda' > list.txt
     ./calcsum list.txt >/dev/null
@@ -53,8 +54,8 @@ if [ ! -f $CHECKPROF/mm/.tmp_mmap.gcda ]; then
     # this is effectively a no-op but ensures the profile data is valid.
     # the tool appeared in gcc 6.1 and seems to be able fixup
     # any missing info in the gcda files.
-    # It is bit unclear is calcsum still required: gcov-tool seems to do
-    # exactly same thing as calcsum.
+    # To improve the profile data consistency multiple data sets should
+    # be merged and then scaled.
     echo 'Postprocesing profile data with kgcov-tool..'
     $KGCCVER/kgcov-tool rewrite -s 1.0 -o /tmp/kernelPGOdump/ /tmp/kernelPGOdump/
 
@@ -101,6 +102,13 @@ echo 'See build.log for normal KBuild log so far.'
 taskset -pc 0-7 $$ >/dev/null
 {
 make $@ V=2 CC=$KGCCVER/kgcc AR=$KGCCVER/kgcc-ar NM=$KGCCVER/kgcc-nm KCFLAGS="-fopt-info-optall=optimize.log";
+if [ $? -eq 0 ]; then
+    echo 'Profile Optimized Kernel build okay!'
+    exit 0
+else
+    echo 'Kernel build failed. Stop.'
+    exit 2;
+fi
 } >build.log 2>&1
 
 echo "Build of profile optimized kernel is done."
diff --git a/kernel/gcov/base.c b/kernel/gcov/base.c
index 7520d706ca56..e21e464e2612 100644
--- a/kernel/gcov/base.c
+++ b/kernel/gcov/base.c
@@ -100,18 +100,15 @@ void __gcov_merge_time_profile(gcov_type *counters, unsigned int n_counters)
 EXPORT_SYMBOL(__gcov_merge_time_profile);
 
 /*
- * To keep GCOV machinery state consistent
- * use simple atomic var as try-lock.
- * Profile counter update funcs below simply ignore
- * the update attempt if they fail acquire the lock.
- * Failing to get the try-lock should not be a problem
- * because over time the attempt is retried.
+ * A try-lock must be used to keep kgcov state consistent.
+ * We still need atomic ops for modifying the counter data,
+ * this lock protects only the RMW sequences below.
  */
 static atomic_t __gcov_lock_mtx = { 0 };
 
 static inline int __gcov_try_lock(void)
 {
-	return atomic_cmpxchg(&__gcov_lock_mtx, 0, 1) == 0;
+	return !(atomic_cmpxchg(&__gcov_lock_mtx, 0, 1));
 }
 
 static inline void __gcov_unlock(void)
@@ -122,27 +119,32 @@ static inline void __gcov_unlock(void)
 static inline void __gcov_one_value_profiler_body(gcov_type *counters,
 						  gcov_arg value)
 {
-	gcov_arg val = GCOV_COUNTER_READ(counters+1);
-	gcov_arg tst = GCOV_COUNTER_READ(counters);
-	if( tst == value) {
-		GCOV_COUNTER_INC(counters+1);
-	} else if (val == 0) {
-		GCOV_COUNTER_SET(counters+1, 1);
-		GCOV_COUNTER_SET(counters, value);
-	} else {
-		GCOV_COUNTER_DEC(counters+1);
+	gcov_arg val, tst;
+	while(1) {
+		val = GCOV_COUNTER_READ(counters+1);
+		tst = GCOV_COUNTER_READ(counters);
+		if( tst == value) {
+			GCOV_COUNTER_INC(counters+1);
+		} else if (val == 0) {
+			if(GCOV_COUNTER_CMPXCHG(counters+1, val, 1) == val) {
+				GCOV_COUNTER_SET(counters, value);
+			} else
+				continue; /* retry */
+		} else {
+			GCOV_COUNTER_DEC(counters+1);
+		}
+		GCOV_COUNTER_INC(counters+2);
+		return;
 	}
-	GCOV_COUNTER_INC(counters+2);
 }
 
 void __gcov_indirect_call_profiler(gcov_type *counter, gcov_arg value,
 				   void *cur_func, void *callee_func)
 {
 	if(__gcov_try_lock()) {
-		if (cur_func == callee_func)
-			__gcov_one_value_profiler_body(counter, value);
-		__gcov_unlock();
-	}
+	if (cur_func == callee_func)
+		__gcov_one_value_profiler_body(counter, value);
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_indirect_call_profiler);
 
@@ -157,8 +159,7 @@ void __gcov_indirect_call_profiler_v2(gcov_arg value, void *cur_func)
 		if (cur_func == __gcov_indirect_call_callee)
 			__gcov_one_value_profiler_body(__gcov_indirect_call_counters,
 							value);
-		__gcov_unlock();
-	}
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_indirect_call_profiler_v2);
 
@@ -168,9 +169,8 @@ EXPORT_SYMBOL(__gcov_time_profiler_counter);
 void __gcov_ior_profiler(gcov_type *counters, gcov_arg value)
 {
 	if(__gcov_try_lock()) {
-		GCOV_COUNTER_BITOR(counters, value);
-		__gcov_unlock();
-	}
+	GCOV_COUNTER_BITOR(counters, value);
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_ior_profiler);
 
@@ -184,10 +184,9 @@ EXPORT_SYMBOL(__gcov_ior_profiler_atomic);
 void __gcov_average_profiler(gcov_type *counters, gcov_arg value)
 {
 	if(__gcov_try_lock()) {
-		GCOV_COUNTER_ADD(counters, value);
-		GCOV_COUNTER_INC(counters+1);
-		__gcov_unlock();
-	}
+	GCOV_COUNTER_ADD(counters, value);
+	GCOV_COUNTER_INC(counters+1);
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_average_profiler);
 
@@ -201,9 +200,8 @@ EXPORT_SYMBOL(__gcov_average_profiler_atomic);
 void __gcov_one_value_profiler(gcov_type *counters, gcov_arg value)
 {
 	if(__gcov_try_lock()) {
-		__gcov_one_value_profiler_body(counters, value);
-		__gcov_unlock();
-	}
+	__gcov_one_value_profiler_body(counters, value);
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_one_value_profiler);
 
@@ -211,9 +209,8 @@ EXPORT_SYMBOL(__gcov_one_value_profiler);
 void __gcov_one_value_profiler_atomic(gcov_type *counters, gcov_arg value)
 {
 	if(__gcov_try_lock()) {
-		__gcov_one_value_profiler_body(counters, value);
-		__gcov_unlock();
-	}
+	__gcov_one_value_profiler_body(counters, value);
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_one_value_profiler_atomic);
 
@@ -222,14 +219,13 @@ void __gcov_interval_profiler(gcov_type *counters, gcov_arg value,
 {
 	long long delta = value - start;
 	if(__gcov_try_lock()) {
-		if (delta < 0)
-			GCOV_COUNTER_INC(counters + steps + 1);
-		else if (delta >= steps)
-			GCOV_COUNTER_INC(counters + steps);
-		else
-			GCOV_COUNTER_INC(counters + delta);
-		__gcov_unlock();
-	}
+	if (delta < 0)
+		GCOV_COUNTER_INC(counters + steps + 1);
+	else if (delta >= steps)
+		GCOV_COUNTER_INC(counters + steps);
+	else
+		GCOV_COUNTER_INC(counters + delta);
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_interval_profiler);
 
@@ -244,12 +240,11 @@ EXPORT_SYMBOL(__gcov_interval_profiler_atomic);
 void __gcov_pow2_profiler(gcov_type *counters, gcov_arg value)
 {
 	if(__gcov_try_lock()) {
-		if (value & (value - 1))
-			GCOV_COUNTER_INC(counters);
-		else
-			GCOV_COUNTER_INC(counters+1);
-		__gcov_unlock();
-	}
+	if (value & (value - 1))
+		GCOV_COUNTER_INC(counters);
+	else
+		GCOV_COUNTER_INC(counters+1);
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_pow2_profiler);
 
@@ -260,7 +255,7 @@ void __gcov_pow2_profiler_atomic(gcov_type *counters, gcov_arg value)
 }
 EXPORT_SYMBOL(__gcov_pow2_profiler_atomic);
 
-/* not sure if gcc uses this profiler at all */
+/* gcc does not use this profiler by default yet */
 static inline void __gcov_topn_value_profiler_body(gcov_type * counters, gcov_arg value)
 {
 	unsigned int i, j, found = 0, have_zero_count = 0;
@@ -347,11 +342,12 @@ EXPORT_SYMBOL(__gcov_indirect_call_topn_callee);
 
 void __gcov_indirect_call_topn_profiler(gcov_arg value, void* cur_func)
 {
+	if(__gcov_try_lock()) {
 	void *callee_func = __gcov_indirect_call_topn_callee;
-	if (cur_func == callee_func && __gcov_try_lock()) {
+	if (cur_func == callee_func) {
 		__gcov_topn_value_profiler_body (__gcov_indirect_call_topn_counters, value);
-		__gcov_unlock();
 	}
+	__gcov_unlock(); }
 }
 EXPORT_SYMBOL(__gcov_indirect_call_topn_profiler);
 
diff --git a/kernel/gcov/gcov.h b/kernel/gcov/gcov.h
index 37eca46fbd07..59b38e61f573 100644
--- a/kernel/gcov/gcov.h
+++ b/kernel/gcov/gcov.h
@@ -42,18 +42,34 @@
  * These macros wrap all operations done on the profile counters
  * allowing to inject code.
  */
-typedef long long gcov_type;
+#if BITS_PER_LONG >= 64
+typedef atomic64_t gcov_type;
 typedef long long gcov_arg;
-#define GCOV_COUNTER_READ(X) (*(X))
-#define GCOV_COUNTER_SET(X, A) (*(X) = (A))
-#define GCOV_COUNTER_INC(X) ((*(X))++)
-#define GCOV_COUNTER_DEC(X) ((*(X))--)
-#define GCOV_COUNTER_ADD(X, A) (*(X) += (A))
-#define GCOV_COUNTER_SUB(X, A) (*(X) -= (A))
-#define GCOV_COUNTER_FETCH_ADD(X, A) (*(X) += (A))
-#define GCOV_COUNTER_FETCH_SUB(X, A) (*(X) -= (A))
-#define GCOV_COUNTER_BITOR(X, A) (*(X) |= (A))
-#define GCOV_COUNTER_CMPXCHG(X, B, A) (*(X) == (B) ? *(X) = (A) : *(X))
+#define GCOV_COUNTER_READ(X) atomic64_read((X))
+#define GCOV_COUNTER_SET(X, A) atomic64_set((X), (A))
+#define GCOV_COUNTER_INC(X) atomic64_inc((X))
+#define GCOV_COUNTER_DEC(X) atomic64_dec((X))
+#define GCOV_COUNTER_ADD(X, A) atomic64_add((A), (X))
+#define GCOV_COUNTER_SUB(X, A) atomic64_sub((A), (X))
+#define GCOV_COUNTER_FETCH_ADD(X, A) atomic64_fetch_add((A), (X))
+#define GCOV_COUNTER_FETCH_SUB(X, A) atomic64_fetch_sub((A), (X))
+#define GCOV_COUNTER_BITOR(X, A) atomic64_or((A), (X))
+#define GCOV_COUNTER_CMPXCHG(X, B, A) atomic64_cmpxchg((X), (B), (A))
+#else
+typedef atomic_t gcov_type;
+typedef long gcov_arg;
+#define GCOV_COUNTER_READ(X) ((gcov_type) atomic_read((atomic_t*)&(X)))
+#define GCOV_COUNTER_SET(X, A) atomic_set((atomic_t*)&(X), (A))
+#define GCOV_COUNTER_INC(X) atomic_inc((atomic_t*)&(X))
+#define GCOV_COUNTER_DEC(X) atomic_dec((atomic_t*)&(X))
+#define GCOV_COUNTER_ADD(X, A) atomic_add((A), (atomic_t*)&(X))
+#define GCOV_COUNTER_SUB(X, A) atomic_sub((A), (atomic_t*)&(X))
+#define GCOV_COUNTER_FETCH_ADD(X, A) atomic_fetch_add((A), (atomic_t*)&(X))
+#define GCOV_COUNTER_FETCH_SUB(X, A) atomic_fetch_sub((A), (atomic_t*)&(X))
+#define GCOV_COUNTER_BITOR(X, A) atomic_or((A), (atomic_t*)&(X))
+#define GCOV_COUNTER_CMPXCHG(X, B, A) atomic_cmpxchg((X), (B), (A))
+#endif
+
 
 /* Opaque gcov_info. The gcov structures can change as for example in gcc 4.7 so
  * we cannot use full definition here and they need to be placed in gcc specific
-- 
2.19.2

